{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Nr9PrcNq8s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "#import xgboost as XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier  # Example model\n",
        "from scipy.stats import randint  # For random distribution options\n",
        "from sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,classification_report,confusion_matrix\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "Zp54UBpnO8oQ",
        "outputId": "50752ae5-fff6-449a-b808-fd71854a9c06"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ed049e5-38fb-4907-9805-f9f0aafbf2d3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ed049e5-38fb-4907-9805-f9f0aafbf2d3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BreastCancerData.csv to BreastCancerData.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'breast-cancer.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d24942892e91>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moriginal_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'breast-cancer.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# count the NA values in the array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'breast-cancer.csv'"
          ]
        }
      ],
      "source": [
        "# 1 upload file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "original_df=pd.read_csv('.csv')\n",
        "# count the NA values in the array\n",
        "pd.isnull(original_df).sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5HLIgJV8QDA"
      },
      "outputs": [],
      "source": [
        "# 1.1 generate df\n",
        "df=original_df.drop(['id'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyPbZ0ErEY4Q"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "plt.figure(figsize=(30, 10))\n",
        "for i, feature in enumerate (df.columns,1):\n",
        "    plt.subplot(4,11,i)\n",
        "    sns.histplot((df[df['diagnosis'] == 'M'])[feature], color='#E74C3C', alpha=0.5, kde=True)\n",
        "    sns.histplot((df[df['diagnosis'] == 'B'])[feature], color='#12436D', alpha=0.5, kde=True)\n",
        "    plt.title(f'Distribute {feature}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bsvT0UQEXWb"
      },
      "outputs": [],
      "source": [
        "# 2 get equal amount for M&B\n",
        "m=(original_df['diagnosis']==\"M\").sum() # malignant\n",
        "b=(original_df['diagnosis']==\"B\").sum() # benign\n",
        "# to get equail amount of control vs malignant\n",
        "sample_size = max(0, b - m)\n",
        "# Drop the rows after sampling\n",
        "df.drop(df[original_df['diagnosis'] == \"M\"].sample(n=sample_size, random_state=0).index, inplace=True)\n",
        "y_colname='diagnosis'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Q3PKMKe1JVdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q16yhAXTEY7g"
      },
      "outputs": [],
      "source": [
        "df['p1'] = (df['perimeter_mean'] ** 2) / (4 * np.pi) - (np.pi * df['radius_mean'] ** 2)\n",
        "df['p2'] = (df['perimeter_se'] ** 2) / (4 * np.pi) - (np.pi * df['radius_se'] ** 2)\n",
        "df['p3'] = (df['perimeter_worst'] ** 2) / (4 * np.pi) - (np.pi * df['radius_worst'] ** 2)\n",
        "# PCA reduction\n",
        "pca = PCA(n_components=3)\n",
        "df['c1'] = pca.fit_transform(df[['compactness_worst','concavity_worst','compactness_mean','concavity_mean']])[:, 0]\n",
        "df['c2'] = pca.fit_transform(df[['perimeter_worst', 'radius_worst','area_worst','area_mean','radius_mean','perimeter_mean']])[:, 0]\n",
        "#df['c3'] = pca.fit_transform(df[['concave_points_worst', 'concave_points_mean','p1','p3']])[:, 0]\n",
        "#df['c4'] = pca.fit_transform(df[['fractal_dimension_worst', 'fractal_dimension_mean','fractal_dimension_se','concave_points_worst','compactness_se','concavity_se']])[:, 0]\n",
        "#df['c11'] = pca.fit_transform(df[['compactness_worst','concavity_worst','compactness_mean','concavity_mean']])[:, 1]\n",
        "#df['c12'] = pca.fit_transform(df[['perimeter_worst', 'radius_worst','area_worst','area_mean','radius_mean','perimeter_mean']])[:, 1]\n",
        "#df['diagnosis'] = df['diagnosis'].replace(('B','M'),(1,0))\n",
        "\"\"\"\n",
        "matrix = df.corr()\n",
        "sns.clustermap(matrix, annot=True, cmap=\"coolwarm\", figsize=(24, 20))\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKIMLwpQEY-x"
      },
      "outputs": [],
      "source": [
        "# according to the observation above:\n",
        "# 1 mean of radius, perimeter, radius_worst, and perimeter_worst columns are dropped due to redundant\n",
        "# 2 many columns, like symmetry-se, (low corelation) weak corelated columns are dropped\n",
        "# 3 drop those columns with the same information (take a guess), like just keep one in area_mean and area_worst\n",
        "#df=df[['diagnosis', 'area_mean', 'concavity_mean', 'concave_points_mean','perimeter_se', 'area_se',\n",
        "#      'concave_points_se', 'area_worst', 'compactness_worst', 'concavity_worst','concave_points_worst']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8j2QMegPA65"
      },
      "outputs": [],
      "source": [
        "ds=df[['radius_worst', 'area_worst', 'perimeter_worst', 'radius_mean',y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD1vsndBGvuD"
      },
      "outputs": [],
      "source": [
        "# reduce to 16 for GXBoost\n",
        "ds=df[['perimeter_worst', 'concavity_worst', 'area_mean', 'concavity_se', 'concave_points_worst', 'area_worst', 'texture_se', 'smoothness_se', 'radius_worst', 'concave_points_mean', 'compactness_mean', 'smoothness_mean', 'radius_se', 'radius_mean', 'concavity_mean', 'symmetry_mean',y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F0yYu5tPx5C"
      },
      "outputs": [],
      "source": [
        "ds=df[['perimeter_worst', 'concavity_worst', 'area_mean', 'concavity_se', y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4KvzWISEcjQ"
      },
      "outputs": [],
      "source": [
        "# reduce to 9 for random forest\n",
        "ds=df[['radius_worst','perimeter_worst','area_worst','concave_points_worst',y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce to 16 for random forest\n",
        "ds=df[['area_worst','perimeter_worst','radius_worst','concave_points_mean',y_colname]]"
      ],
      "metadata": {
        "id": "IqX2ZC5ZhgWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD0-Hfw6EAv1"
      },
      "outputs": [],
      "source": [
        "# without engineered\n",
        "ds=df[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
        "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
        "       'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
        "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
        "       'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se',\n",
        "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
        "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "       'compactness_worst', 'concavity_worst', 'concave_points_worst',\n",
        "       'symmetry_worst', 'fractal_dimension_worst', y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with engineered\n",
        "ds=df"
      ],
      "metadata": {
        "id": "AgPrUmLEjZo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=ds.drop(\"diagnosis\", axis=1)\n",
        "y=ds.diagnosis"
      ],
      "metadata": {
        "id": "PcdiddlLisuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = X.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "# scale X & y\n",
        "X_scaler = StandardScaler()\n",
        "y_scaler = StandardScaler()\n",
        "X_scaled = X_scaler.fit_transform(X[numeric_cols])\n",
        "X[numeric_cols] = X_scaled\n",
        "# If y is a pandas Series or DataFrame, use .values to get the numpy array\n",
        "# If y is already a numpy array, use it directly\n",
        "if isinstance(y, (pd.Series, pd.DataFrame)): y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "elif isinstance(y, np.ndarray): y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "#y = pd.DataFrame({y_colname: y})\n",
        "# get back the column names\n",
        "def apply_feature_group_types(df, feature_groups):\n",
        "    column_type_mapping = {}\n",
        "    # Assuming feature_groups is a list of dictionaries:\n",
        "    for group in feature_groups:  # Iterate through each group in the list\n",
        "        # Check if the group is a dictionary and has the necessary keys\n",
        "        if isinstance(group, dict) and 'group_name' in group and 'columns' in group:\n",
        "            group_name = group['group_name']\n",
        "            columns = group['columns']\n",
        "            for column in columns:\n",
        "                if column in X.columns:\n",
        "                    column_type_mapping[column] = data_types.get(column)\n",
        "    df = df.astype(column_type_mapping)\n",
        "    return df\n",
        "# Apply the types\n",
        "\n",
        "print(\"null\",df.isnull().sum().sum(),\"; inf\",df.isin([np.inf, -np.inf]).sum().sum(),\"; X shape\",X.shape,'; y shape',y.shape,end='')"
      ],
      "metadata": {
        "id": "PDjINefiidCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avzwfYacEZBp"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(ds.drop(\"diagnosis\", axis=1),\n",
        "                                                 ds.diagnosis,\n",
        "                                                 test_size= 1/4,\n",
        "                                                 random_state=0,\n",
        "                                                 stratify= ds.diagnosis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJZUA0YtBbfI"
      },
      "outputs": [],
      "source": [
        "#scaler = StandardScaler()\n",
        "#data_scalered= scaler.fit_transform(X_train);X_train = pd.DataFrame(data_scalered, columns=X_train.columns)\n",
        "#data_scalered= scaler.fit_transform(X_test);X_test = pd.DataFrame(data_scalered, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iNFm7dTEZEQ"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=0)\n",
        "\"\"\"\n",
        "# Hyperparameter grid for RandomForestClassifier\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "grid_rf = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=0),\n",
        "    param_grid_rf,\n",
        "    cv=5,\n",
        "    scoring='accuracy'  # Change the scoring metric for classification\n",
        ")\n",
        "grid_rf.fit(X_train, y_train)\n",
        "model1=RandomForestClassifier(**grid_rf.best_params_,\n",
        "            random_state=0)\n",
        "\"\"\"\n",
        "\n",
        "model1=RandomForestClassifier(\n",
        "    bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200,\n",
        "            random_state=0)\n",
        "\n",
        "model1.fit(X_train, y_train)\n",
        "predicciones = model1.predict(X_test)\n",
        "# Define the parameter distribution\n",
        "#parametros = {\n",
        "#    'n_estimators': [100],  # Increased max number of trees\n",
        "#    'max_depth': [12],   # More depth options\n",
        "#    'min_samples_split': [5, 10],   # Adjust split criteria\n",
        "#    'min_samples_leaf': [2, 4],      # More leaf options\n",
        "#    'max_features': ['sqrt', 'log2', 0.5], # Add a float option for feature selection\n",
        "#    'bootstrap': [True, False]              # Keep bootstrap options\n",
        "#}\n",
        "\n",
        "# Configure the randomized search with cross-validation\n",
        "#randomized_search_cv = RandomizedSearchCV(**grid_rf.best_params_,\n",
        "#            random_state=0)\n",
        "\n",
        "# Train the model\n",
        "#RF_class = randomized_search_cv.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "#print(\"Mejores hiperparámetros encontrados:\")\n",
        "#print(randomized_search_cv.best_params_)\n",
        "\n",
        "# Use the best model to make predictions\n",
        "#mejor_modelo = RF_class.best_estimator_\n",
        "#predicciones = mejor_modelo.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRGzXjx0dBMV"
      },
      "outputs": [],
      "source": [
        "#grid_rf.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka6DHFkfEZJw"
      },
      "outputs": [],
      "source": [
        "test_acc = accuracy_score(y_test, predicciones)\n",
        "test_f1 = f1_score(y_test, predicciones,  average = 'weighted')\n",
        "test_recall = recall_score(y_test, predicciones,   average = 'weighted')\n",
        "test_precision1 = precision_score(y_test , predicciones, average = 'weighted')\n",
        "print(\"Resultados RandomForest\")\n",
        "print(f\"ACC = {test_acc}\")\n",
        "print(f\"F1 = {test_f1}\")\n",
        "print(f\"Recall = {test_recall}\")\n",
        "print(f\"Precision = {test_precision1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dodU_wD7EZMg"
      },
      "outputs": [],
      "source": [
        "print(\"RandomForest Results\")\n",
        "print(classification_report(y_test, predicciones))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inZiIJY1EZPj"
      },
      "outputs": [],
      "source": [
        "def get_importances (mejor_modelo, X_train):\n",
        "    feat_importances = pd.DataFrame(mejor_modelo.feature_importances_, index = X_train.columns, columns = [\"Importance\"])\n",
        "    feat_importances.sort_values(by = \"Importance\", ascending = False, inplace = True)\n",
        "    feat_importances.plot(kind = \"bar\", color = \"blue\", figsize = (8,6))\n",
        "    print(feat_importances)\n",
        "\n",
        "get_importances(model1,X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jctcc0EYjsRC"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(model1, X_test, y_test, cv=5, scoring='accuracy')  # Puedes cambiar 'accuracy' por otra métrica\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Accuracy: {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEJsEu3KjsY0"
      },
      "outputs": [],
      "source": [
        "class_counts = np.bincount(y_train)\n",
        "class_weight = class_counts[0] / class_counts[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFXkPNbujsbs"
      },
      "outputs": [],
      "source": [
        "#model2 = GridSearchCV.XGBClassifier(use_label_encoder=False,scale_pos_weight=class_weight, eval_metric='logloss')\n",
        "# parameter\n",
        "#params = {\n",
        "#    'booster': 'gbtree',\n",
        "#    'max_depth': 6,\n",
        "#    'learning_rate': 0.1,\n",
        "#    'n_estimators': 100,\n",
        "#    'subsample': 0.8,\n",
        "#    'colsample_bytree': 0.8,\n",
        "#    'objective': 'binary:logistic',\n",
        "#    'eval_metric': 'logloss'\n",
        "#}\n",
        "\"\"\"\n",
        "# Hyperparameter grid for XGBClassifier\n",
        "gparam_rid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_xgb = GridSearchCV(XGBClassifier(random_state=0, enable_categorical=True),\n",
        "                        gparam_rid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_xgb.fit(X_train, y_train)\n",
        "\n",
        "model2=XGBClassifier(\n",
        "            **grid_xgb.best_params_,\n",
        "            random_state=0,\n",
        "            enable_categorical=True)\n",
        "\"\"\"\n",
        "\n",
        "model2 = XGBClassifier(\n",
        "    colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6,\n",
        "    random_state=0,\n",
        "    enable_categorical=True\n",
        ")\n",
        "\n",
        "model2.fit(X_train, y_train)\n",
        "test_pred2 = model2.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHCyBAtbdtn5"
      },
      "outputs": [],
      "source": [
        "#grid_xgb.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jpwHQU9jseV"
      },
      "outputs": [],
      "source": [
        "test_acc = accuracy_score(y_test, test_pred2)\n",
        "test_f1 = f1_score(y_test, test_pred2,  average = 'weighted')\n",
        "test_recall = recall_score(y_test, test_pred2,   average = 'weighted')\n",
        "test_precision2 = precision_score(y_test , test_pred2, average = 'weighted')\n",
        "print(\"XGBoostClassifier Results\")\n",
        "print(f\"ACC = {test_acc}\")\n",
        "print(f\"F1 = {test_f1}\")\n",
        "print(f\"Recall = {test_recall}\")\n",
        "print(f\"Precision = {test_precision2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUI6B5tGjsg2"
      },
      "outputs": [],
      "source": [
        "get_importances(model2,X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKj2828mjsjk"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(model2, X_test, y_test, cv=5, scoring='accuracy')  # Puedes cambiar 'accuracy' por otra métrica\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Accuracy: {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model2.predict(X_test)\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2rWIU335rU7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaQIfshTjsmO"
      },
      "outputs": [],
      "source": [
        "#with open('RF_class.pkl', 'wb') as file:\n",
        "#    pickle.dump(mejor_modelo, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT50MmTpkzme"
      },
      "outputs": [],
      "source": [
        "#with open(\"RF_class.pkl\", \"rb\") as file:\n",
        "#    mejor_modelo = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXedQThIkzpl"
      },
      "outputs": [],
      "source": [
        "#inputs = X_test.iloc[:,:].values.tolist()\n",
        "# [\n",
        "#df.iloc[0:1,1:].values[0].tolist(),\n",
        "#df.iloc[2:3,1:].values[0].tolist()\n",
        "#]\n",
        "#[17.99,10.38,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871],\n",
        "#[20.57,17.77,1326,0.08474,0.07864,0.0869,0.07017,0.1812,0.05667]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGz6IoXSAC8t"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Confusion Matrix\n",
        "plt.figure(figsize=(15, 7))\n",
        "for i, (model, y_pred) in enumerate(zip([model, model2], [test_pred,test_pred2])):\n",
        "  # Use the index (i) for the subplot number\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  plt.subplot(2, 4, i+1 )  # Start from 1\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title(f'Confusion Matrix for {model.__class__.__name__}'); plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aYMtzEekzsy"
      },
      "outputs": [],
      "source": [
        "\"\"\"predicciones = mejor_modelo.predict(inputs)\n",
        "for i, pred in enumerate(predicciones):\n",
        "    print(f\"Input {i+1}: {inputs[i]} -> Predicción: {pred}\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfzvaahdkzvF"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Obtener las probabilidades en lugar de predicciones crudas\n",
        "predicciones = mejor_modelo.predict_proba(inputs)\n",
        "\n",
        "for i, pred in enumerate(predicciones):\n",
        "    clase_predicha = pred.argmax()\n",
        "    # if clase_predicha ==  1:\n",
        "    #     clase_predicha_str = 'Non demented'\n",
        "    # else:\n",
        "    #     clase_predicha_str = \"Demented\"\n",
        "\n",
        "    probabilidad = pred[clase_predicha] * 100\n",
        "\n",
        "    print(f\"Input {i+1}: Prediction Class: {clase_predicha}, veracity: {probabilidad:.2f}%\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RswXZQ2sJI9b"
      },
      "outputs": [],
      "source": [
        "#predicciones.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2zz3cxMnbRK"
      },
      "outputs": [],
      "source": [
        "\"\"\"ds=np.array(df);y=ds[:,0:1];ds=ds[:,1:];\n",
        "pca_c = PCA();pca_c.fit(ds);\n",
        "cumulative_variance = np.cumsum(pca_c.explained_variance_ratio_)\n",
        "# Perform PCA for n_components=2\n",
        "pca = PCA(n_components=2);scaler = StandardScaler();X_pca = pca.fit_transform(scaler.fit_transform(df))\n",
        "# Create side by side plots\n",
        "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))  # 1 row, 2 columns for side by side\n",
        "# Left plot: Cumulative Explained Variance\n",
        "ax1.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
        "# Right plot: PCA result\n",
        "scatter1 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)\n",
        "ax1.set_title('Cumulative Explained Variance')\n",
        "ax1.set_xlabel('Principal Component')\n",
        "ax1.set_ylabel('Cumulative Explained Variance')\n",
        "ax1.set_xticks(range(1, len(cumulative_variance) + 1))\n",
        "ax1.axhline(y=0.9, color='r', linestyle='--')  # Reference line for 90%\n",
        "ax1.grid(True)\n",
        "ax2.set_xlabel('Principal Component 1')\n",
        "ax2.set_ylabel('Principal Component 2')\n",
        "ax2.set_title('PCA of Disease vs Control Data')\n",
        "ax2.grid(True)\n",
        "# Add colorbar for the right plot\n",
        "fig1.colorbar(scatter1, ax=ax2, label='Group (0 = Control, 1 = Disease)')\n",
        "# Show the figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Get the PCA loadings\n",
        "loadings = pca.components_.T;np.round(loadings,decimals=4)\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}