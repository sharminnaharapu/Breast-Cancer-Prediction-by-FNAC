{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M6Nr9PrcNq8s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "#import xgboost as XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier  # Example model\n",
        "from scipy.stats import randint  # For random distribution options\n",
        "from sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,classification_report,confusion_matrix\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zp54UBpnO8oQ",
        "outputId": "c93560e5-9e2d-4029-cfb0-f2bbbc406dad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-84742bc6-7501-489f-ac24-ddac10fa3da3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-84742bc6-7501-489f-ac24-ddac10fa3da3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                         0\n",
              "diagnosis                  0\n",
              "radius_mean                0\n",
              "texture_mean               0\n",
              "perimeter_mean             0\n",
              "area_mean                  0\n",
              "smoothness_mean            0\n",
              "compactness_mean           0\n",
              "concavity_mean             0\n",
              "concave_points_mean        0\n",
              "symmetry_mean              0\n",
              "fractal_dimension_mean     0\n",
              "radius_se                  0\n",
              "texture_se                 0\n",
              "perimeter_se               0\n",
              "area_se                    0\n",
              "smoothness_se              0\n",
              "compactness_se             0\n",
              "concavity_se               0\n",
              "concave_points_se          0\n",
              "symmetry_se                0\n",
              "fractal_dimension_se       0\n",
              "radius_worst               0\n",
              "texture_worst              0\n",
              "perimeter_worst            0\n",
              "area_worst                 0\n",
              "smoothness_worst           0\n",
              "compactness_worst          0\n",
              "concavity_worst            0\n",
              "concave_points_worst       0\n",
              "symmetry_worst             0\n",
              "fractal_dimension_worst    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diagnosis</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave_points_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave_points_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave_points_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# 1 upload file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "original_df=pd.read_csv('BreastCancerData.csv')\n",
        "# count the NA values in the array\n",
        "pd.isnull(original_df).sum(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B5HLIgJV8QDA"
      },
      "outputs": [],
      "source": [
        "# 1.1 generate df\n",
        "df=original_df.drop(['id'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QyPbZ0ErEY4Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "246070c2-5ee6-43ea-ccd1-3df41c7fe43d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nplt.figure(figsize=(30, 10))\\nfor i, feature in enumerate (df.columns,1):\\n    plt.subplot(4,11,i)\\n    sns.histplot((df[df['diagnosis'] == 'M'])[feature], color='#E74C3C', alpha=0.5, kde=True)\\n    sns.histplot((df[df['diagnosis'] == 'B'])[feature], color='#12436D', alpha=0.5, kde=True)\\n    plt.title(f'Distribute {feature}')\\nplt.tight_layout()\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\"\"\"\n",
        "plt.figure(figsize=(30, 10))\n",
        "for i, feature in enumerate (df.columns,1):\n",
        "    plt.subplot(4,11,i)\n",
        "    sns.histplot((df[df['diagnosis'] == 'M'])[feature], color='#E74C3C', alpha=0.5, kde=True)\n",
        "    sns.histplot((df[df['diagnosis'] == 'B'])[feature], color='#12436D', alpha=0.5, kde=True)\n",
        "    plt.title(f'Distribute {feature}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9bsvT0UQEXWb"
      },
      "outputs": [],
      "source": [
        "# 2 get equal amount for M&B\n",
        "m=(original_df['diagnosis']==\"M\").sum() # malignant\n",
        "b=(original_df['diagnosis']==\"B\").sum() # benign\n",
        "# to get equail amount of control vs malignant\n",
        "sample_size = max(0, b - m)\n",
        "# Drop the rows after sampling\n",
        "df.drop(df[original_df['diagnosis'] == \"M\"].sample(n=sample_size, random_state=0).index, inplace=True)\n",
        "y_colname='diagnosis'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Q3PKMKe1JVdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc526fb1-b76a-4aa9-80c1-02045452315c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
              "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
              "       'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
              "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
              "       'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se',\n",
              "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
              "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
              "       'compactness_worst', 'concavity_worst', 'concave_points_worst',\n",
              "       'symmetry_worst', 'fractal_dimension_worst'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q16yhAXTEY7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6528b2f6-f390-46d8-f0d4-bcbce3ca35d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmatrix = df.corr()\\nsns.clustermap(matrix, annot=True, cmap=\"coolwarm\", figsize=(24, 20))\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df['p1'] = (df['perimeter_mean'] ** 2) / (4 * np.pi) - (np.pi * df['radius_mean'] ** 2)\n",
        "df['p2'] = (df['perimeter_se'] ** 2) / (4 * np.pi) - (np.pi * df['radius_se'] ** 2)\n",
        "df['p3'] = (df['perimeter_worst'] ** 2) / (4 * np.pi) - (np.pi * df['radius_worst'] ** 2)\n",
        "# PCA reduction\n",
        "pca = PCA(n_components=3)\n",
        "df['c1'] = pca.fit_transform(df[['compactness_worst','concavity_worst','compactness_mean','concavity_mean']])[:, 0]\n",
        "df['c2'] = pca.fit_transform(df[['perimeter_worst', 'radius_worst','area_worst','area_mean','radius_mean','perimeter_mean']])[:, 0]\n",
        "#df['c3'] = pca.fit_transform(df[['concave_points_worst', 'concave_points_mean','p1','p3']])[:, 0]\n",
        "#df['c4'] = pca.fit_transform(df[['fractal_dimension_worst', 'fractal_dimension_mean','fractal_dimension_se','concave_points_worst','compactness_se','concavity_se']])[:, 0]\n",
        "#df['c11'] = pca.fit_transform(df[['compactness_worst','concavity_worst','compactness_mean','concavity_mean']])[:, 1]\n",
        "#df['c12'] = pca.fit_transform(df[['perimeter_worst', 'radius_worst','area_worst','area_mean','radius_mean','perimeter_mean']])[:, 1]\n",
        "#df['diagnosis'] = df['diagnosis'].replace(('B','M'),(1,0))\n",
        "\"\"\"\n",
        "matrix = df.corr()\n",
        "sns.clustermap(matrix, annot=True, cmap=\"coolwarm\", figsize=(24, 20))\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pKIMLwpQEY-x"
      },
      "outputs": [],
      "source": [
        "# according to the observation above:\n",
        "# 1 mean of radius, perimeter, radius_worst, and perimeter_worst columns are dropped due to redundant\n",
        "# 2 many columns, like symmetry-se, (low corelation) weak corelated columns are dropped\n",
        "# 3 drop those columns with the same information (take a guess), like just keep one in area_mean and area_worst\n",
        "#df=df[['diagnosis', 'area_mean', 'concavity_mean', 'concave_points_mean','perimeter_se', 'area_se',\n",
        "#      'concave_points_se', 'area_worst', 'compactness_worst', 'concavity_worst','concave_points_worst']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A8j2QMegPA65"
      },
      "outputs": [],
      "source": [
        "ds=df[['radius_worst', 'area_worst', 'perimeter_worst', 'radius_mean',y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wD1vsndBGvuD"
      },
      "outputs": [],
      "source": [
        "# reduce to 16 for GXBoost\n",
        "ds=df[['perimeter_worst', 'concavity_worst', 'area_mean', 'concavity_se', 'concave_points_worst', 'area_worst', 'texture_se', 'smoothness_se', 'radius_worst', 'concave_points_mean', 'compactness_mean', 'smoothness_mean', 'radius_se', 'radius_mean', 'concavity_mean', 'symmetry_mean',y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5F0yYu5tPx5C"
      },
      "outputs": [],
      "source": [
        "ds=df[['perimeter_worst', 'concavity_worst', 'area_mean', 'concavity_se', y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R4KvzWISEcjQ"
      },
      "outputs": [],
      "source": [
        "# reduce to 9 for random forest\n",
        "ds=df[['radius_worst','perimeter_worst','area_worst','concave_points_worst',y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce to 16 for random forest\n",
        "ds=df[['area_worst','perimeter_worst','radius_worst','concave_points_mean',y_colname]]"
      ],
      "metadata": {
        "id": "IqX2ZC5ZhgWH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kD0-Hfw6EAv1"
      },
      "outputs": [],
      "source": [
        "# without engineered\n",
        "ds=df[['radius_mean', 'texture_mean', 'perimeter_mean',\n",
        "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
        "       'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
        "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
        "       'compactness_se', 'concavity_se', 'concave_points_se', 'symmetry_se',\n",
        "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
        "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "       'compactness_worst', 'concavity_worst', 'concave_points_worst',\n",
        "       'symmetry_worst', 'fractal_dimension_worst', y_colname]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with engineered\n",
        "ds=df"
      ],
      "metadata": {
        "id": "AgPrUmLEjZo9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=ds.drop(\"diagnosis\", axis=1)\n",
        "y=ds.diagnosis"
      ],
      "metadata": {
        "id": "PcdiddlLisuF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = X.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "# scale X & y\n",
        "X_scaler = StandardScaler()\n",
        "y_scaler = StandardScaler()\n",
        "X_scaled = X_scaler.fit_transform(X[numeric_cols])\n",
        "X[numeric_cols] = X_scaled\n",
        "# If y is a pandas Series or DataFrame, use .values to get the numpy array\n",
        "# If y is already a numpy array, use it directly\n",
        "if isinstance(y, (pd.Series, pd.DataFrame)): y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
        "elif isinstance(y, np.ndarray): y_scaled = y_scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "#y = pd.DataFrame({y_colname: y})\n",
        "# get back the column names\n",
        "def apply_feature_group_types(df, feature_groups):\n",
        "    column_type_mapping = {}\n",
        "    # Assuming feature_groups is a list of dictionaries:\n",
        "    for group in feature_groups:  # Iterate through each group in the list\n",
        "        # Check if the group is a dictionary and has the necessary keys\n",
        "        if isinstance(group, dict) and 'group_name' in group and 'columns' in group:\n",
        "            group_name = group['group_name']\n",
        "            columns = group['columns']\n",
        "            for column in columns:\n",
        "                if column in X.columns:\n",
        "                    column_type_mapping[column] = data_types.get(column)\n",
        "    df = df.astype(column_type_mapping)\n",
        "    return df\n",
        "# Apply the types\n",
        "\n",
        "print(\"null\",df.isnull().sum().sum(),\"; inf\",df.isin([np.inf, -np.inf]).sum().sum(),\"; X shape\",X.shape,'; y shape',y.shape,end='')"
      ],
      "metadata": {
        "id": "PDjINefiidCO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "3a951a0a-f1be-4b3f-8b1b-784ec2bc41e9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'M'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-25ceb4f153e2>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# If y is a pandas Series or DataFrame, use .values to get the numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# If y is already a numpy array, use it directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#y = pd.DataFrame({y_colname: y})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \"\"\"\n\u001b[1;32m    913\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'M'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avzwfYacEZBp"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(ds.drop(\"diagnosis\", axis=1),\n",
        "                                                 ds.diagnosis,\n",
        "                                                 test_size= 1/4,\n",
        "                                                 random_state=0,\n",
        "                                                 stratify= ds.diagnosis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJZUA0YtBbfI"
      },
      "outputs": [],
      "source": [
        "#scaler = StandardScaler()\n",
        "#data_scalered= scaler.fit_transform(X_train);X_train = pd.DataFrame(data_scalered, columns=X_train.columns)\n",
        "#data_scalered= scaler.fit_transform(X_test);X_test = pd.DataFrame(data_scalered, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iNFm7dTEZEQ"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=0)\n",
        "\"\"\"\n",
        "# Hyperparameter grid for RandomForestClassifier\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "grid_rf = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=0),\n",
        "    param_grid_rf,\n",
        "    cv=5,\n",
        "    scoring='accuracy'  # Change the scoring metric for classification\n",
        ")\n",
        "grid_rf.fit(X_train, y_train)\n",
        "model1=RandomForestClassifier(**grid_rf.best_params_,\n",
        "            random_state=0)\n",
        "\"\"\"\n",
        "\n",
        "model1=RandomForestClassifier(\n",
        "    bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200,\n",
        "            random_state=0)\n",
        "\n",
        "model1.fit(X_train, y_train)\n",
        "predicciones = model1.predict(X_test)\n",
        "# Define the parameter distribution\n",
        "#parametros = {\n",
        "#    'n_estimators': [100],  # Increased max number of trees\n",
        "#    'max_depth': [12],   # More depth options\n",
        "#    'min_samples_split': [5, 10],   # Adjust split criteria\n",
        "#    'min_samples_leaf': [2, 4],      # More leaf options\n",
        "#    'max_features': ['sqrt', 'log2', 0.5], # Add a float option for feature selection\n",
        "#    'bootstrap': [True, False]              # Keep bootstrap options\n",
        "#}\n",
        "\n",
        "# Configure the randomized search with cross-validation\n",
        "#randomized_search_cv = RandomizedSearchCV(**grid_rf.best_params_,\n",
        "#            random_state=0)\n",
        "\n",
        "# Train the model\n",
        "#RF_class = randomized_search_cv.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "#print(\"Mejores hiperpar√°metros encontrados:\")\n",
        "#print(randomized_search_cv.best_params_)\n",
        "\n",
        "# Use the best model to make predictions\n",
        "#mejor_modelo = RF_class.best_estimator_\n",
        "#predicciones = mejor_modelo.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRGzXjx0dBMV"
      },
      "outputs": [],
      "source": [
        "#grid_rf.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka6DHFkfEZJw"
      },
      "outputs": [],
      "source": [
        "test_acc = accuracy_score(y_test, predicciones)\n",
        "test_f1 = f1_score(y_test, predicciones,  average = 'weighted')\n",
        "test_recall = recall_score(y_test, predicciones,   average = 'weighted')\n",
        "test_precision1 = precision_score(y_test , predicciones, average = 'weighted')\n",
        "print(\"Resultados RandomForest\")\n",
        "print(f\"ACC = {test_acc}\")\n",
        "print(f\"F1 = {test_f1}\")\n",
        "print(f\"Recall = {test_recall}\")\n",
        "print(f\"Precision = {test_precision1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dodU_wD7EZMg"
      },
      "outputs": [],
      "source": [
        "print(\"RandomForest Results\")\n",
        "print(classification_report(y_test, predicciones))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inZiIJY1EZPj"
      },
      "outputs": [],
      "source": [
        "def get_importances (mejor_modelo, X_train):\n",
        "    feat_importances = pd.DataFrame(mejor_modelo.feature_importances_, index = X_train.columns, columns = [\"Importance\"])\n",
        "    feat_importances.sort_values(by = \"Importance\", ascending = False, inplace = True)\n",
        "    feat_importances.plot(kind = \"bar\", color = \"blue\", figsize = (8,6))\n",
        "    print(feat_importances)\n",
        "\n",
        "get_importances(model1,X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jctcc0EYjsRC"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(model1, X_test, y_test, cv=5, scoring='accuracy')  # Puedes cambiar 'accuracy' por otra m√©trica\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Accuracy: {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEJsEu3KjsY0"
      },
      "outputs": [],
      "source": [
        "class_counts = np.bincount(y_train)\n",
        "class_weight = class_counts[0] / class_counts[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFXkPNbujsbs"
      },
      "outputs": [],
      "source": [
        "#model2 = GridSearchCV.XGBClassifier(use_label_encoder=False,scale_pos_weight=class_weight, eval_metric='logloss')\n",
        "# parameter\n",
        "#params = {\n",
        "#    'booster': 'gbtree',\n",
        "#    'max_depth': 6,\n",
        "#    'learning_rate': 0.1,\n",
        "#    'n_estimators': 100,\n",
        "#    'subsample': 0.8,\n",
        "#    'colsample_bytree': 0.8,\n",
        "#    'objective': 'binary:logistic',\n",
        "#    'eval_metric': 'logloss'\n",
        "#}\n",
        "\"\"\"\n",
        "# Hyperparameter grid for XGBClassifier\n",
        "gparam_rid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_xgb = GridSearchCV(XGBClassifier(random_state=0, enable_categorical=True),\n",
        "                        gparam_rid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_xgb.fit(X_train, y_train)\n",
        "\n",
        "model2=XGBClassifier(\n",
        "            **grid_xgb.best_params_,\n",
        "            random_state=0,\n",
        "            enable_categorical=True)\n",
        "\"\"\"\n",
        "\n",
        "model2 = XGBClassifier(\n",
        "    colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6,\n",
        "    random_state=0,\n",
        "    enable_categorical=True\n",
        ")\n",
        "\n",
        "model2.fit(X_train, y_train)\n",
        "test_pred2 = model2.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHCyBAtbdtn5"
      },
      "outputs": [],
      "source": [
        "#grid_xgb.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jpwHQU9jseV"
      },
      "outputs": [],
      "source": [
        "test_acc = accuracy_score(y_test, test_pred2)\n",
        "test_f1 = f1_score(y_test, test_pred2,  average = 'weighted')\n",
        "test_recall = recall_score(y_test, test_pred2,   average = 'weighted')\n",
        "test_precision2 = precision_score(y_test , test_pred2, average = 'weighted')\n",
        "print(\"XGBoostClassifier Results\")\n",
        "print(f\"ACC = {test_acc}\")\n",
        "print(f\"F1 = {test_f1}\")\n",
        "print(f\"Recall = {test_recall}\")\n",
        "print(f\"Precision = {test_precision2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUI6B5tGjsg2"
      },
      "outputs": [],
      "source": [
        "get_importances(model2,X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKj2828mjsjk"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(model2, X_test, y_test, cv=5, scoring='accuracy')  # Puedes cambiar 'accuracy' por otra m√©trica\n",
        "print(f\"Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Mean Accuracy: {scores.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model2.predict(X_test)\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2rWIU335rU7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaQIfshTjsmO"
      },
      "outputs": [],
      "source": [
        "#with open('RF_class.pkl', 'wb') as file:\n",
        "#    pickle.dump(mejor_modelo, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT50MmTpkzme"
      },
      "outputs": [],
      "source": [
        "#with open(\"RF_class.pkl\", \"rb\") as file:\n",
        "#    mejor_modelo = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXedQThIkzpl"
      },
      "outputs": [],
      "source": [
        "#inputs = X_test.iloc[:,:].values.tolist()\n",
        "# [\n",
        "#df.iloc[0:1,1:].values[0].tolist(),\n",
        "#df.iloc[2:3,1:].values[0].tolist()\n",
        "#]\n",
        "#[17.99,10.38,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871],\n",
        "#[20.57,17.77,1326,0.08474,0.07864,0.0869,0.07017,0.1812,0.05667]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGz6IoXSAC8t"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Confusion Matrix\n",
        "plt.figure(figsize=(15, 7))\n",
        "for i, (model, y_pred) in enumerate(zip([model, model2], [test_pred,test_pred2])):\n",
        "  # Use the index (i) for the subplot number\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  plt.subplot(2, 4, i+1 )  # Start from 1\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title(f'Confusion Matrix for {model.__class__.__name__}'); plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aYMtzEekzsy"
      },
      "outputs": [],
      "source": [
        "\"\"\"predicciones = mejor_modelo.predict(inputs)\n",
        "for i, pred in enumerate(predicciones):\n",
        "    print(f\"Input {i+1}: {inputs[i]} -> Predicci√≥n: {pred}\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfzvaahdkzvF"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Obtener las probabilidades en lugar de predicciones crudas\n",
        "predicciones = mejor_modelo.predict_proba(inputs)\n",
        "\n",
        "for i, pred in enumerate(predicciones):\n",
        "    clase_predicha = pred.argmax()\n",
        "    # if clase_predicha ==  1:\n",
        "    #     clase_predicha_str = 'Non demented'\n",
        "    # else:\n",
        "    #     clase_predicha_str = \"Demented\"\n",
        "\n",
        "    probabilidad = pred[clase_predicha] * 100\n",
        "\n",
        "    print(f\"Input {i+1}: Prediction Class: {clase_predicha}, veracity: {probabilidad:.2f}%\")\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RswXZQ2sJI9b"
      },
      "outputs": [],
      "source": [
        "#predicciones.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2zz3cxMnbRK"
      },
      "outputs": [],
      "source": [
        "\"\"\"ds=np.array(df);y=ds[:,0:1];ds=ds[:,1:];\n",
        "pca_c = PCA();pca_c.fit(ds);\n",
        "cumulative_variance = np.cumsum(pca_c.explained_variance_ratio_)\n",
        "# Perform PCA for n_components=2\n",
        "pca = PCA(n_components=2);scaler = StandardScaler();X_pca = pca.fit_transform(scaler.fit_transform(df))\n",
        "# Create side by side plots\n",
        "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))  # 1 row, 2 columns for side by side\n",
        "# Left plot: Cumulative Explained Variance\n",
        "ax1.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
        "# Right plot: PCA result\n",
        "scatter1 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)\n",
        "ax1.set_title('Cumulative Explained Variance')\n",
        "ax1.set_xlabel('Principal Component')\n",
        "ax1.set_ylabel('Cumulative Explained Variance')\n",
        "ax1.set_xticks(range(1, len(cumulative_variance) + 1))\n",
        "ax1.axhline(y=0.9, color='r', linestyle='--')  # Reference line for 90%\n",
        "ax1.grid(True)\n",
        "ax2.set_xlabel('Principal Component 1')\n",
        "ax2.set_ylabel('Principal Component 2')\n",
        "ax2.set_title('PCA of Disease vs Control Data')\n",
        "ax2.grid(True)\n",
        "# Add colorbar for the right plot\n",
        "fig1.colorbar(scatter1, ax=ax2, label='Group (0 = Control, 1 = Disease)')\n",
        "# Show the figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Get the PCA loadings\n",
        "loadings = pca.components_.T;np.round(loadings,decimals=4)\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}